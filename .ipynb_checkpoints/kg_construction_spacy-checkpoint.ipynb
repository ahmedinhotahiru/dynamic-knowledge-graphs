{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pymupdf in c:\\users\\ahmed issah tahiru\\desktop\\cmu-africa\\professor okeyo research\\kgs updating codebase\\venv\\lib\\site-packages (1.24.13)\n"
     ]
    }
   ],
   "source": [
    "!pip install pymupdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Extract Text from the PDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We’ll use the PyMuPDF library (fitz) to extract text from the PDF. This library is effective for handling structured text, such as reports and tables, commonly found in survey data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extracting text from PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz  # PyMuPDF\n",
    "\n",
    "# Function to extract text from each page in the PDF\n",
    "def extract_text_from_pdf(file_path):\n",
    "    # Open the PDF file\n",
    "    document = fitz.open(file_path)\n",
    "    text_data = []\n",
    "\n",
    "    # Iterate through each page\n",
    "    for page_num in range(document.page_count):\n",
    "        page = document[page_num]\n",
    "        page_text = page.get_text()  # Extract text from page\n",
    "        text_data.append(page_text)\n",
    "\n",
    "    document.close()\n",
    "    return text_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the PDF file\n",
    "file_path = 'files/Final_SAS 2023_Annual Report.pdf'\n",
    "pdf_text = extract_text_from_pdf(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Page 1 ---\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "The Republic of Rwanda \n",
      "SEASONAL \n",
      "AGRICULTURAL SURVEY \n",
      "2023 \n",
      "ANNUAL REPORT \n",
      "December 2023 \n",
      "\n",
      "--- Page 2 ---\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "SEASONAL AGRICULTURAL SURVEY \n",
      " \n",
      "2023 \n",
      " \n",
      "ANNUAL REPORT \n",
      "\n",
      "--- Page 3 ---\n",
      "National Institute of Statistics of Rwanda (NISR) \n",
      " i \n",
      "EXECUTIVE SUMMARY \n",
      "This is the annual report for the Seasonal Agricultural Survey (SAS) conducted by the National Institute of \n",
      "Statistics of Rwanda (NISR) for the agricultural year 2022/2023, which covers three primary agricultural \n",
      "seasons in Rwanda. The main agricultural seasons include Season A, spanned from September 2022 to \n",
      "February 2023, Season B which started from March to June 2023, and Season C which started from July to \n",
      "Septembe\n"
     ]
    }
   ],
   "source": [
    "# Check the first few pages to see the extracted text\n",
    "for i, page in enumerate(pdf_text[:3]):\n",
    "    print(f\"--- Page {i+1} ---\")\n",
    "    print(page[:500])  # Print first 500 characters for preview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Text Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We’ll implement the following preprocessing steps:\n",
    "\n",
    "Remove Extra Spaces and Line Breaks: To make the text easier to work with.\n",
    "\n",
    "Split Text into Sentences: This will help with processing the text sentence by sentence during entity extraction.\n",
    "\n",
    "Normalize Case and Remove Unwanted Characters: For consistent analysis, we’ll standardize the case and remove characters like page numbers, special symbols, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Ahmed Issah\n",
      "[nltk_data]     Tahiru\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "# download nltk toketizer\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to C:\\Users\\Ahmed Issah\n",
      "[nltk_data]     Tahiru\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt_tab.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to preprocess text\n",
    "def preprocess_text(text_data):\n",
    "    processed_text = []\n",
    "\n",
    "    for page_text in text_data:\n",
    "        # Remove any extraneous whitespace and newlines\n",
    "        page_text = page_text.replace('\\n', ' ').strip()\n",
    "\n",
    "        # Remove unwanted characters like page numbers or table of contents markers\n",
    "        page_text = re.sub(r'\\bPage\\s\\d+\\b', '', page_text)\n",
    "        page_text = re.sub(r'[^a-zA-Z0-9\\s.,]', '', page_text)\n",
    "\n",
    "        # Convert text to lowercase\n",
    "        page_text = page_text.lower()\n",
    "\n",
    "        # Tokenize text into sentences\n",
    "        sentences = sent_tokenize(page_text)\n",
    "\n",
    "        # Store cleaned sentences\n",
    "        processed_text.extend(sentences)\n",
    "\n",
    "    return processed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence 1: the republic of rwanda  seasonal  agricultural survey  2023  annual report  december 2023\n",
      "Sentence 2: seasonal agricultural survey    2023    annual report\n",
      "Sentence 3: national institute of statistics of rwanda nisr   i  executive summary  this is the annual report for the seasonal agricultural survey sas conducted by the national institute of  statistics of rwanda nisr for the agricultural year 20222023, which covers three primary agricultural  seasons in rwanda.\n",
      "Sentence 4: the main agricultural seasons include season a, spanned from september 2022 to  february 2023, season b which started from march to june 2023, and season c which started from july to  september 2023.   data sources   sas is a primarybased data survey, combining area frame and a list frame.\n",
      "Sentence 5: it covered 1,200 segments and  345 large scale farmers.\n",
      "Sentence 6: it is conducted in two distinct phases screening and harvesting phases.\n",
      "Sentence 7: the  screening phase covers grown crops data, estimates on cultivated area, and erosion control measures on  plots.\n",
      "Sentence 8: the harvesting phase covers interviews from farmers who grew crops in that specific season.\n",
      "Sentence 9: it covered  22,971 plots and 345 largescale farms for season a, and 23,007 plots and 333 largescale farmers for  season b, specifically targeting plots with crops ready for harvest in respective seasons.\n",
      "Sentence 10: estimates for major  crops are prepared mainly to give timely and current district and national totals and averages.\n",
      "Sentence 11: survey primary  data are supplemented by tea and coffee data from national agricultural export development board naeb  collected through the routine activities of monitoring coffee and tea production, and coffee census conducted  every five years.\n",
      "Sentence 12: key findings   a comprehensive summary of the primary indicators assessed during sas 2023 is presented in table 0. it  includes crop production, yield estimates, land use, use of agricultural inputs and agricultural practices.\n",
      "Sentence 13: land use   the total country land area is estimated at 2.377 million of hectares for which 1.367 million hectares 57.5  of total country land is used for agriculture.\n",
      "Sentence 14: in 2023 season a, 1 million hectares were used for seasonal  crops, 0.5 million hectares were covered by permanent crops, while 0.126 million hectares were used for  permanent pasture.\n",
      "Sentence 15: likewise, in season b, the total country land area is estimated at 2.377 million of hectares  for which 1.345 million hectares 56.6 of total country land is used for agriculture.\n",
      "Sentence 16: in addition, 983,000  hectares were used for seasonal crops, 540,000 hectares were covered by permanent crops, while 124,000  hectares were used for permanent pasture.\n",
      "Sentence 17: use of inputs   commonly used agricultural inputs include improved seeds, organic and chemical fertilizers, as well as  pesticidesfungicides.\n",
      "Sentence 18: the 2023 results indicate that 37.1 percent of farmers in season a, 20.8 percent in  season b, and 20.7 percent in season c used improved seeds.\n",
      "Sentence 19: in season a, 87.9 percent of farmers applied  organic fertilizer, 83.4 percent in season b, and 83.7 percent applied organic fertilizers in season c. in  addition, 59.6 percent of famers applied inorganic fertilizer in season a, 51.6 percent of famers applied\n",
      "Sentence 20: seasonal agricultural survey 2023  inorganic fertilizer in season b while 74.5 percent in season c. in regard to use of pesticides, in season a of  2023, 34.5 percent of farmers applied pesticides in season a, 29.7 percent in season b and 69.3 percent in  season c.  agricultural practices   in season a of 2023, 10.3 percent of farmers practiced irrigation compared to 9.6 percent of farmers in  season b and 64.6 percent in season c. in 2023 season a, 92.1 percent of farmers practiced antierosion  activities, compared to 91.6 percent of farmers in season b and 94.8 percent of farmers in season c.\n"
     ]
    }
   ],
   "source": [
    "# Apply preprocessing to the extracted text\n",
    "cleaned_text = preprocess_text(pdf_text)\n",
    "\n",
    "# Display the first few cleaned sentences\n",
    "for i, sentence in enumerate(cleaned_text[:20]):\n",
    "    print(f\"Sentence {i+1}: {sentence}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3: Entity Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We’ll use spaCy, an NLP library that provides pre-trained models for named entity recognition (NER), part-of-speech tagging, and other text processing tasks. We may need to train or fine-tune the model later on for agricultural-specific terms, but for now, we’ll start with spaCy's base model and explore what it extracts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.1 Install spaCy and Download Language Model\n",
    "If not already installed, we’ll install spaCy and download the en_core_web_sm model, which is spaCy's small English language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install spacy\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.2 Extracting Entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# Load spaCy's pre-trained English model\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract entities from the text\n",
    "def extract_entities(text_data):\n",
    "    entities = []\n",
    "\n",
    "    for sentence in text_data:\n",
    "\n",
    "        # Process each sentence using spaCy's NLP pipeline\n",
    "        doc = nlp(sentence)\n",
    "\n",
    "        for ent in doc.ents:\n",
    "            # Append each recognized entity and its label\n",
    "            entities.append((ent.text, ent.label_))\n",
    "\n",
    "    return entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply entity extraction on the cleaned text\n",
    "extracted_entities = extract_entities(cleaned_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entity 1: Text: 'rwanda', Label: GPE\n",
      "Entity 2: Text: '2023  annual', Label: DATE\n",
      "Entity 3: Text: 'december 2023', Label: DATE\n",
      "Entity 4: Text: 'seasonal', Label: DATE\n",
      "Entity 5: Text: 'national institute of statistics', Label: ORG\n",
      "Entity 6: Text: 'rwanda', Label: GPE\n",
      "Entity 7: Text: 'annual', Label: DATE\n",
      "Entity 8: Text: 'the national institute of  statistics', Label: ORG\n",
      "Entity 9: Text: 'rwanda', Label: GPE\n",
      "Entity 10: Text: 'the agricultural year 20222023', Label: DATE\n",
      "Entity 11: Text: 'three', Label: CARDINAL\n",
      "Entity 12: Text: 'rwanda', Label: GPE\n",
      "Entity 13: Text: 'september 2022', Label: DATE\n",
      "Entity 14: Text: 'february 2023, season', Label: DATE\n",
      "Entity 15: Text: 'march to', Label: DATE\n",
      "Entity 16: Text: 'june 2023', Label: DATE\n",
      "Entity 17: Text: 'july', Label: DATE\n",
      "Entity 18: Text: 'september 2023', Label: DATE\n",
      "Entity 19: Text: '1,200', Label: CARDINAL\n",
      "Entity 20: Text: '345', Label: CARDINAL\n"
     ]
    }
   ],
   "source": [
    "# display a sample of extracted entities\n",
    "for i, entity in enumerate(extracted_entities[:20]):\n",
    "    print(f\"Entity {i+1}: Text: '{entity[0]}', Label: {entity[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entity Types to Note for Agricultural Data\n",
    "- ORG: Organizations or institutions (e.g., \"National Institute of Statistics of Rwanda\").\n",
    "- DATE: Dates, which may relate to crop seasons.\n",
    "- GPE/LOC: Geopolitical entities or locations relevant to land use or agricultural regions.\n",
    "- CARDINAL/QUANTITY: Quantities often related to measurements or crop statistics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Relationship Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We’ll analyze the extracted sentences to identify relationships between entities. For instance, relationships like \"maize grows in\" a specific season or \"fertilizer applied to\" certain crops can provide valuable insights for building a structured knowledge graph.\n",
    "\n",
    "We’ll use dependency parsing, which identifies syntactic relationships between words in a sentence. spaCy’s dependency parser will help us capture these relationships, focusing on:\n",
    "\n",
    "- Subject-Verb-Object (SVO) triples: Common in sentences that describe actions, like \"farmers use fertilizers.\"\n",
    "- Prepositional Phrases: Often contain location or temporal data, like \"in season A.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Extracting Relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract relationships from sentences\n",
    "\n",
    "def extract_relationships(text_data):\n",
    "\n",
    "    # container to store extracted relationships\n",
    "    relationships = []\n",
    "\n",
    "    # Loop through each sentence in the text to extract relationships \n",
    "    for sentence in text_data:\n",
    "        doc = nlp(sentence)\n",
    "\n",
    "        # Define placeholders for entities and relationships\n",
    "        subject = None\n",
    "        predicate = None\n",
    "        obj = None\n",
    "\n",
    "        # Dependency parsing to identify SVO structure\n",
    "        for token in doc:\n",
    "\n",
    "            # Find the subject (usually a noun or a compound noun)\n",
    "            if \"subj\" in token.dep_:\n",
    "                subject = token.text\n",
    "\n",
    "            # Fint the object (usually a noun or a compound noun)\n",
    "            elif \"obj\" in token.dep_:\n",
    "                obj = token.text\n",
    "\n",
    "            # Find the main verb (predicate of the sentence)\n",
    "            elif token.pos_ == \"VERB\":\n",
    "                # Use lemma for consistent verbs (e.g., 'use' vs 'used')\n",
    "                predicate = token.lemma_\n",
    "\n",
    "        # If SVO structure is found, store the relationship\n",
    "        if subject and predicate and obj:\n",
    "            relationships.append((subject, predicate, obj))\n",
    "\n",
    "    return relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply relationship extraction on cleaned text\n",
    "extracted_relationships = extract_relationships(cleaned_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relationship 1: Subject: 'which', Predicate: 'cover', Object: 'rwanda'\n",
      "Relationship 2: Subject: 'sas', Predicate: 'combine', Object: 'frame'\n",
      "Relationship 3: Subject: 'it', Predicate: 'cover', Object: 'segments'\n",
      "Relationship 4: Subject: 'it', Predicate: 'screen', Object: 'phases'\n",
      "Relationship 5: Subject: 'phase', Predicate: 'cultivate', Object: 'plots'\n",
      "Relationship 6: Subject: 'who', Predicate: 'grow', Object: 'season'\n",
      "Relationship 7: Subject: 'it', Predicate: 'target', Object: 'seasons'\n",
      "Relationship 8: Subject: 'estimates', Predicate: 'give', Object: 'district'\n",
      "Relationship 9: Subject: 'census', Predicate: 'conduct', Object: 'years'\n",
      "Relationship 10: Subject: 'it', Predicate: 'include', Object: 'inputs'\n",
      "Relationship 11: Subject: '57.5', Predicate: 'use', Object: 'agriculture'\n",
      "Relationship 12: Subject: 'hectares', Predicate: 'use', Object: 'pasture'\n",
      "Relationship 13: Subject: '56.6', Predicate: 'use', Object: 'agriculture'\n",
      "Relationship 14: Subject: 'hectares', Predicate: 'use', Object: 'pasture'\n",
      "Relationship 15: Subject: 'use', Predicate: 'improve', Object: 'seeds'\n",
      "Relationship 16: Subject: 'c', Predicate: 'use', Object: 'seeds'\n",
      "Relationship 17: Subject: 'percent', Predicate: 'apply', Object: 'famers'\n",
      "Relationship 18: Subject: 'percent', Predicate: 'compare', Object: 'c.'\n",
      "Relationship 19: Subject: 'indicators', Predicate: 'maize', Object: 'beer'\n",
      "Relationship 20: Subject: 'summary', Predicate: 'kgha', Object: 'fruits'\n"
     ]
    }
   ],
   "source": [
    "# Display a sample of extracted relationships\n",
    "for i, relationship in enumerate(extracted_relationships[:20]):\n",
    "    print(f\"Relationship {i+1}: Subject: '{relationship[0]}', Predicate: '{relationship[1]}', Object: '{relationship[2]}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Building the DKG with NetworkX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We’ll use the extracted entities and relationships to create a structured knowledge graph that models the agricultural information.\n",
    "\n",
    "To build the knowledge graph, we’ll use the NetworkX library in Python. This will allow us to represent entities as nodes and relationships as edges, creating a graph that can be easily updated and queried."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install networkx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Initialize an empty directed graph\n",
    "G = nx.DiGraph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to build the knowledge graph from entities and relationships\n",
    "def build_knowledge_graph(entities, relationships):\n",
    "\n",
    "    # Add entities as nodes\n",
    "    for entity, entity_type in entities:\n",
    "        G.add_node(entity, label=entity_type)\n",
    "\n",
    "    # Add relationships as edges\n",
    "    for subject, predicate, obj in relationships:\n",
    "        G.add_edge(subject, obj, label=predicate)\n",
    "\n",
    "    return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the graph using extracted entities and relationships\n",
    "knowledge_graph = build_knowledge_graph(extracted_entities, extracted_relationships)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw the graph\n",
    "plt.figure(figsize=(12, 12))\n",
    "# Layout for visualization\n",
    "pos = nx.spring_layout(knowledge_graph, seed=42)\n",
    "nx.draw(knowledge_graph, pos, with_labels=True, node_size=3000, node_color=\"skyblue\", font_size=10, font_weight=\"bold\", edge_color=\"gray\")\n",
    "edge_labels = nx.get_edge_attributes(knowledge_graph, \"label\")\n",
    "nx.draw_networkx_edge_labels(knowledge_graph, pos, edge_labels=edge_labels, font_color=\"red\")\n",
    "plt.title(\"Agricultural Knowledge Graph\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
